{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering\n",
    "## Credit Scoring Model Project\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Handle missing values systematically\n",
    "- Create domain-based features using business knowledge\n",
    "- Encode categorical variables appropriately\n",
    "- Scale and normalize numerical features\n",
    "- Perform feature selection\n",
    "- Prepare data for modeling\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "Feature engineering is the process of using domain knowledge to create features that make machine learning algorithms work better. It's often the difference between a mediocre model and a great one!\n",
    "\n",
    "**Quote:** \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" - Andrew Ng\n",
    "\n",
    "Let's build powerful features!\n",
    "\n",
    "## üì¶ Import Libraries and Utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:56:33.162247Z",
     "iopub.status.busy": "2025-12-06T11:56:33.161732Z",
     "iopub.status.idle": "2025-12-06T11:56:37.589593Z",
     "shell.execute_reply": "2025-12-06T11:56:37.587645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our custom utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.data_preprocessing import (\n",
    "    load_data,\n",
    "    analyze_missing_values,\n",
    "    handle_missing_values,\n",
    "    validate_data_quality\n",
    ")\n",
    "from src.domain_features import create_domain_features\n",
    "from src.feature_engineering import encode_categorical_features, clean_column_names, scale_features\n",
    "from src.feature_selection import select_features\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"[OK] All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìÇ Load Data\n",
    "\n",
    "We\\'ll load the data we explored in the EDA notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T11:56:37.653270Z",
     "iopub.status.busy": "2025-12-06T11:56:37.652037Z",
     "iopub.status.idle": "2025-12-06T12:00:46.909274Z",
     "shell.execute_reply": "2025-12-06T12:00:46.906726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\shahu\\OPEN CLASSROOMS\\PROJET 6\\Scoring_Model\\notebooks\\..\\data\n",
      "\n",
      "================================================================================\n",
      "LOADING AND AGGREGATING ALL DATA SOURCES\n",
      "================================================================================\n",
      "\n",
      "1. Loading main application tables...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train: (307511, 122), Test: (48744, 121)\n",
      "\n",
      "2. Loading bureau data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bureau: (1716428, 17)\n",
      "   Processing bureau_balance.csv in chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 1 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 2 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 3 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 4 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 5 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 6 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 7 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 8 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 9 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 10 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 11 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 12 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 13 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 14 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 15 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 16 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 17 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 18 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 19 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 20 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 21 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 22 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 23 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 24 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 25 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 26 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 27 (1,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 28 (299,925 rows)\n",
      "   Combining 28 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bureau Balance aggregated: (817395, 5)\n",
      "  Aggregating bureau data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created 37 bureau features\n",
      "\n",
      "3. Loading previous applications...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Previous applications: (1670214, 37)\n",
      "  Aggregating previous applications...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created 56 previous application features\n",
      "\n",
      "4. Loading POS/cash balances...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   POS/cash: (10001358, 8)\n",
      "  Aggregating POS/cash balances...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created 20 POS/cash features\n",
      "\n",
      "5. Loading credit card balances...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Credit card: (3840312, 23)\n",
      "  Aggregating credit card balances...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Created 52 credit card features\n",
      "\n",
      "6. Loading installment payments...\n",
      "   Processing installments_payments.csv in chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 1 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 2 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 3 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 4 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 5 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 6 (2,000,000 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Processed chunk 7 (1,605,401 rows)\n",
      "   Combining 7 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Installments aggregated: (339587, 32)\n",
      "    Created 31 installment features\n",
      "\n",
      "7. Merging all aggregated features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] Final shapes:\n",
      "   Train: (307511, 318)\n",
      "   Test: (48744, 317)\n",
      "   Total features: 316 (excluding SK_ID_CURR and TARGET)\n",
      "================================================================================\n",
      "\n",
      "[OK] Data validation passed\n",
      "Training set: (307511, 318)\n",
      "Test set: (48744, 317)\n",
      "\n",
      "Target distribution:\n",
      "TARGET\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load data using our utility\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "print(f\"Training set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(train_df['TARGET'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîç Handle Missing Values\n",
    "\n",
    "**Strategy:**\n",
    "1. Identify features with excessive missing values (>70%) ‚Üí Consider dropping\n",
    "2. Create missing indicators for important features\n",
    "3. Impute remaining missing values appropriately\n",
    "\n",
    "**Educational Note:**\n",
    "Different imputation strategies work for different scenarios:\n",
    "- **Median:** For numerical features with outliers (robust)\n",
    "- **Mean:** For numerical features with normal distribution\n",
    "- **Mode:** For categorical features\n",
    "- **Constant:** When missingness has meaning (e.g., 0 for no car)\n",
    "- **Missing Indicator:** Preserve information about missingness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:00:46.914675Z",
     "iopub.status.busy": "2025-12-06T12:00:46.913942Z",
     "iopub.status.idle": "2025-12-06T12:00:48.457375Z",
     "shell.execute_reply": "2025-12-06T12:00:48.455390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "================================================================================\n",
      "Total features: 318\n",
      "Features with missing > 0%: 263\n",
      "\\nTop features with missing values:\n",
      "                            column  missing_count  missing_percent   dtype\n",
      " PREV_RATE_INTEREST_PRIVILEGED_MIN         302902        98.501192 float64\n",
      "   PREV_RATE_INTEREST_PRIMARY_MEAN         302902        98.501192 float64\n",
      "    PREV_RATE_INTEREST_PRIMARY_MIN         302902        98.501192 float64\n",
      "    PREV_RATE_INTEREST_PRIMARY_MAX         302902        98.501192 float64\n",
      "PREV_RATE_INTEREST_PRIVILEGED_MEAN         302902        98.501192 float64\n",
      " PREV_RATE_INTEREST_PRIVILEGED_MAX         302902        98.501192 float64\n",
      "        CC_AMT_PAYMENT_CURRENT_MIN         246451        80.143800 float64\n",
      "        CC_AMT_PAYMENT_CURRENT_MAX         246451        80.143800 float64\n",
      "       CC_AMT_PAYMENT_CURRENT_MEAN         246451        80.143800 float64\n",
      "  CC_AMT_DRAWINGS_POS_CURRENT_MEAN         246371        80.117784 float64\n",
      "  CC_AMT_DRAWINGS_ATM_CURRENT_MEAN         246371        80.117784 float64\n",
      "            BUREAU_AMT_ANNUITY_MAX         227502        73.981744 float64\n",
      "           BUREAU_AMT_ANNUITY_MEAN         227502        73.981744 float64\n",
      "                 CC_SK_DPD_DEF_MAX         220606        71.739222 float64\n",
      "                 CC_SK_DPD_DEF_SUM         220606        71.739222 float64\n",
      "\\nüìä Severity Breakdown:\n",
      "   High (>50%): 101 features\n",
      "   Medium (20-50%): 11 features\n",
      "   Low (<20%): 151 features\n",
      "\n",
      "[DECISION SUMMARY]\n",
      "High missing (>70%): 60 features - CONSIDER DROPPING\n",
      "Medium missing (20-70%): 52 features - CREATE INDICATORS + IMPUTE\n",
      "Low missing (<20%): 151 features - IMPUTE ONLY\n",
      "\n",
      "Dropping 60 features with >70% missing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training shape: (307511, 258)\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing values\n",
    "missing_summary = analyze_missing_values(train_df, threshold=0)\n",
    "\n",
    "# Separate features by missing percentage\n",
    "high_missing = missing_summary[missing_summary['missing_percent'] > 70]['column'].tolist()\n",
    "medium_missing = missing_summary[(missing_summary['missing_percent'] > 20) &\n",
    "                                 (missing_summary['missing_percent'] <= 70)]['column'].tolist()\n",
    "low_missing = missing_summary[(missing_summary['missing_percent'] > 0) &\n",
    "                              (missing_summary['missing_percent'] <= 20)]['column'].tolist()\n",
    "\n",
    "print(f\"\\n[DECISION SUMMARY]\")\n",
    "print(f\"High missing (>70%): {len(high_missing)} features - CONSIDER DROPPING\")\n",
    "print(f\"Medium missing (20-70%): {len(medium_missing)} features - CREATE INDICATORS + IMPUTE\")\n",
    "print(f\"Low missing (<20%): {len(low_missing)} features - IMPUTE ONLY\")\n",
    "\n",
    "# Let\\'s drop very sparse features\n",
    "print(f\"\\nDropping {len(high_missing)} features with >70% missing...\")\n",
    "train_df = train_df.drop(columns=high_missing)\n",
    "test_df = test_df.drop(columns=[col for col in high_missing if col in test_df.columns])\n",
    "\n",
    "print(f\"New training shape: {train_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéØ Create Domain-Based Features\n",
    "\n",
    "**Domain Knowledge in Credit Scoring:**\n",
    "Key financial ratios and indicators matter:\n",
    "- **Debt-to-Income Ratio:** How much debt relative to income?\n",
    "- **Credit Utilization:** How much of available credit is used?\n",
    "- **Payment Burden:** Can they afford the payments?\n",
    "- **Age/Employment Stability:** Risk indicators\n",
    "\n",
    "Let\\'s create features that capture these concepts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:00:48.461898Z",
     "iopub.status.busy": "2025-12-06T12:00:48.461453Z",
     "iopub.status.idle": "2025-12-06T12:00:49.591653Z",
     "shell.execute_reply": "2025-12-06T12:00:49.589776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating domain features...\n",
      "  [OK] Age features created\n",
      "  [OK] Employment features created\n",
      "  [OK] Income features created\n",
      "  [OK] Credit features created\n",
      "  [OK] Family features created\n",
      "  [OK] Document features created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] External source features created\n",
      "  [OK] Regional features created\n",
      "Creating domain features...\n",
      "  [OK] Age features created\n",
      "  [OK] Employment features created\n",
      "  [OK] Income features created\n",
      "  [OK] Credit features created\n",
      "  [OK] Family features created\n",
      "  [OK] Document features created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] External source features created\n",
      "  [OK] Regional features created\n",
      "================================================================================\n",
      "\n",
      "New shape after feature creation:\n",
      "Training: (307511, 273)\n",
      "Test: (48744, 272)\n"
     ]
    }
   ],
   "source": [
    "# Use our new modular function\n",
    "print(\"=\"*80)\n",
    "train_df = create_domain_features(train_df)\n",
    "test_df = create_domain_features(test_df)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nNew shape after feature creation:\")\n",
    "print(f\"Training: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üè∑Ô∏è Encode Categorical Variables\n",
    "\n",
    "**Encoding Strategies:**\n",
    "\n",
    "1. **Label Encoding:** For ordinal categories (order matters)\n",
    "   - Example: Education level (Low ‚Üí Medium ‚Üí High)\n",
    "\n",
    "2. **One-Hot Encoding:** For nominal categories (no order)\n",
    "   - Example: Contract type (Cash, Revolving)\n",
    "   - Creates binary columns for each category\n",
    "\n",
    "3. **Target Encoding:** For high-cardinality features\n",
    "   - Encode by mean target value per category\n",
    "   - Use with caution (risk of overfitting!)\n",
    "\n",
    "**Rule of Thumb:**\n",
    "- Low cardinality (<10 categories) ‚Üí One-Hot Encoding\n",
    "- High cardinality (>10 categories) ‚Üí Target Encoding or drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:00:49.596287Z",
     "iopub.status.busy": "2025-12-06T12:00:49.595788Z",
     "iopub.status.idle": "2025-12-06T12:00:52.253277Z",
     "shell.execute_reply": "2025-12-06T12:00:52.250900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 categorical columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-hot encoding 14 low-cardinality features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[WARNING] 2 high-cardinality features remain: ['OCCUPATION_TYPE', 'ORGANIZATION_TYPE']\n",
      "Dropping these features as they have too many categories for one-hot encoding...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Dropped 2 high-cardinality features\n",
      "\n",
      "Cleaning column names...\n",
      "\n",
      "Final shape after handling all categoricals:\n",
      "Training: (307511, 307)\n",
      "Test: (48744, 306)\n"
     ]
    }
   ],
   "source": [
    "# Use our new modular function\n",
    "train_df, test_df = encode_categorical_features(train_df, test_df, cardinality_limit=10)\n",
    "\n",
    "# Clean column names for LightGBM compatibility\n",
    "print(\"\\nCleaning column names...\")\n",
    "train_df = clean_column_names(train_df)\n",
    "test_df = clean_column_names(test_df)\n",
    "\n",
    "print(f\"\\nFinal shape after handling all categoricals:\")\n",
    "print(f\"Training: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîß Impute Remaining Missing Values\n",
    "\n",
    "Now we\\'ll impute the remaining missing values using appropriate strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:00:52.258698Z",
     "iopub.status.busy": "2025-12-06T12:00:52.258246Z",
     "iopub.status.idle": "2025-12-06T12:01:05.941728Z",
     "shell.execute_reply": "2025-12-06T12:01:05.938896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: 207\n",
      "  Numerical: 207\n",
      "  Categorical: 0\n",
      "\n",
      "Imputing 207 numerical features with median...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Numerical imputation complete\n",
      "\n",
      "[VERIFICATION]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training missing: 0\n",
      "Test missing: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with missing values\n",
    "missing_cols = train_df.columns[train_df.isnull().any()].tolist()\n",
    "if 'TARGET' in missing_cols:\n",
    "    missing_cols.remove('TARGET')\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "\n",
    "# Separate numerical and categorical\n",
    "numerical_missing = [col for col in missing_cols\n",
    "                     if train_df[col].dtype in ['int64', 'float64']]\n",
    "categorical_missing = [col for col in missing_cols\n",
    "                       if train_df[col].dtype == 'object']\n",
    "\n",
    "print(f\"  Numerical: {len(numerical_missing)}\")\n",
    "print(f\"  Categorical: {len(categorical_missing)}\")\n",
    "\n",
    "# Impute numerical with median\n",
    "if numerical_missing:\n",
    "    print(f\"\\nImputing {len(numerical_missing)} numerical features with median...\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    train_df[numerical_missing] = imputer.fit_transform(train_df[numerical_missing])\n",
    "    test_df[numerical_missing] = imputer.transform(test_df[numerical_missing])\n",
    "    print(\"  [OK] Numerical imputation complete\")\n",
    "\n",
    "# Impute categorical with most frequent\n",
    "if categorical_missing:\n",
    "    print(f\"\\nImputing {len(categorical_missing)} categorical features with mode...\")\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train_df[categorical_missing] = imputer.fit_transform(train_df[categorical_missing])\n",
    "    test_df[categorical_missing] = imputer.transform(test_df[categorical_missing])\n",
    "    print(\"  [OK] Categorical imputation complete\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\n[VERIFICATION]\")\n",
    "print(f\"Training missing: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"Test missing: {test_df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéØ Feature Selection\n",
    "\n",
    "**Why Feature Selection?**\n",
    "1. Reduce overfitting (simpler models generalize better)\n",
    "2. Improve model performance (remove noise)\n",
    "3. Reduce training time\n",
    "4. Improve interpretability\n",
    "\n",
    "**Strategies:**\n",
    "1. Remove low-variance features (constant or near-constant)\n",
    "2. Remove highly correlated features (redundant information)\n",
    "3. Use feature importance from baseline models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:01:05.947141Z",
     "iopub.status.busy": "2025-12-06T12:01:05.946683Z",
     "iopub.status.idle": "2025-12-06T12:02:00.053311Z",
     "shell.execute_reply": "2025-12-06T12:02:00.051066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before selection: 305\n",
      "\n",
      "1. Removing low-variance features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 80 low-variance features to remove\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Removing highly correlated features (>{correlation_threshold})...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 36 highly correlated features to remove\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features after selection: 189\n",
      "[SUCCESS] Removed 116 features\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "y_train = train_df['TARGET']\n",
    "X_test = test_df.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "# Use our new modular function\n",
    "X_train, X_test = select_features(X_train, X_test)\n",
    "\n",
    "print(f\"[SUCCESS] Removed {train_df.shape[1] - X_train.shape[1] - 2} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öñÔ∏è Scale Features\n",
    "\n",
    "**Why Scaling?**\n",
    "- Features have different ranges (income vs children count)\n",
    "- Many ML algorithms are sensitive to feature scales\n",
    "- Required for: Logistic Regression, SVM, Neural Networks\n",
    "- Optional for: Tree-based models (Random Forest, XGBoost)\n",
    "\n",
    "**Scaling Methods:**\n",
    "1. **StandardScaler:** Mean=0, Std=1 (use when data is normally distributed)\n",
    "2. **MinMaxScaler:** Scale to [0, 1] (use when data has outliers)\n",
    "3. **RobustScaler:** Uses median and IQR (robust to outliers)\n",
    "\n",
    "We\\'ll use StandardScaler as it works well for most cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:02:00.058961Z",
     "iopub.status.busy": "2025-12-06T12:02:00.058465Z",
     "iopub.status.idle": "2025-12-06T12:02:05.739130Z",
     "shell.execute_reply": "2025-12-06T12:02:05.737328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features with StandardScaler...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Scaling complete\n",
      "\n",
      "Scaled feature statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  DAYS_BIRTH  \\\n",
      "mean          -0.0              -0.0        -0.0         -0.0         0.0   \n",
      "std            1.0               1.0         1.0          1.0         1.0   \n",
      "\n",
      "      DAYS_EMPLOYED  DAYS_REGISTRATION  DAYS_ID_PUBLISH  OWN_CAR_AGE  \\\n",
      "mean            0.0               -0.0              0.0         -0.0   \n",
      "std             1.0                1.0              1.0          1.0   \n",
      "\n",
      "      FLAG_EMP_PHONE  FLAG_WORK_PHONE  FLAG_PHONE  FLAG_EMAIL  \\\n",
      "mean             0.0              0.0         0.0        -0.0   \n",
      "std              1.0              1.0         1.0         1.0   \n",
      "\n",
      "      CNT_FAM_MEMBERS  REGION_RATING_CLIENT  HOUR_APPR_PROCESS_START  \\\n",
      "mean              0.0                   0.0                     -0.0   \n",
      "std               1.0                   1.0                      1.0   \n",
      "\n",
      "      REG_REGION_NOT_LIVE_REGION  REG_REGION_NOT_WORK_REGION  \\\n",
      "mean                         0.0                         0.0   \n",
      "std                          1.0                         1.0   \n",
      "\n",
      "      LIVE_REGION_NOT_WORK_REGION  REG_CITY_NOT_LIVE_CITY  \\\n",
      "mean                          0.0                     0.0   \n",
      "std                           1.0                     1.0   \n",
      "\n",
      "      REG_CITY_NOT_WORK_CITY  LIVE_CITY_NOT_WORK_CITY  EXT_SOURCE_1  \\\n",
      "mean                     0.0                     -0.0           0.0   \n",
      "std                      1.0                      1.0           1.0   \n",
      "\n",
      "      EXT_SOURCE_2  EXT_SOURCE_3  ELEVATORS_AVG  FLOORSMAX_AVG  \\\n",
      "mean          -0.0           0.0           -0.0           -0.0   \n",
      "std            1.0           1.0            1.0            1.0   \n",
      "\n",
      "      OBS_30_CNT_SOCIAL_CIRCLE  DEF_30_CNT_SOCIAL_CIRCLE  \\\n",
      "mean                      -0.0                       0.0   \n",
      "std                        1.0                       1.0   \n",
      "\n",
      "      DEF_60_CNT_SOCIAL_CIRCLE  DAYS_LAST_PHONE_CHANGE  FLAG_DOCUMENT_3  \\\n",
      "mean                       0.0                    -0.0              0.0   \n",
      "std                        1.0                     1.0              1.0   \n",
      "\n",
      "      FLAG_DOCUMENT_5  FLAG_DOCUMENT_6  FLAG_DOCUMENT_8  \\\n",
      "mean             -0.0             -0.0             -0.0   \n",
      "std               1.0              1.0              1.0   \n",
      "\n",
      "      AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
      "mean                        0.0                        -0.0   \n",
      "std                         1.0                         1.0   \n",
      "\n",
      "      AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
      "mean                        0.0                       -0.0   \n",
      "std                         1.0                        1.0   \n",
      "\n",
      "      AMT_REQ_CREDIT_BUREAU_YEAR  BUREAU_SK_ID_BUREAU_COUNT  \\\n",
      "mean                         0.0                       -0.0   \n",
      "std                          1.0                        1.0   \n",
      "\n",
      "      BUREAU_DAYS_CREDIT_MIN  BUREAU_DAYS_CREDIT_MAX  BUREAU_DAYS_CREDIT_MEAN  \\\n",
      "mean                     0.0                    -0.0                      0.0   \n",
      "std                      1.0                     1.0                      1.0   \n",
      "\n",
      "      BUREAU_DAYS_CREDIT_ENDDATE_MIN  BUREAU_DAYS_CREDIT_ENDDATE_MAX  \\\n",
      "mean                            -0.0                             0.0   \n",
      "std                              1.0                             1.0   \n",
      "\n",
      "      BUREAU_DAYS_CREDIT_ENDDATE_MEAN  BUREAU_DAYS_CREDIT_UPDATE_MIN  \\\n",
      "mean                             -0.0                            0.0   \n",
      "std                               1.0                            1.0   \n",
      "\n",
      "      BUREAU_DAYS_CREDIT_UPDATE_MAX  BUREAU_DAYS_CREDIT_UPDATE_MEAN  ...  \\\n",
      "mean                            0.0                             0.0  ...   \n",
      "std                             1.0                             1.0  ...   \n",
      "\n",
      "      INST_PAYMENT_RATIO_MIN  INST_PAYMENT_RATIO_MAX  INST_PAYMENT_RATIO_MEAN  \\\n",
      "mean                     0.0                    -0.0                      0.0   \n",
      "std                      1.0                     1.0                      1.0   \n",
      "\n",
      "      INST_DPD_MIN  INST_DPD_MAX  INST_DPD_MEAN  INST_IS_LATE_SUM  \\\n",
      "mean           0.0          -0.0            0.0              -0.0   \n",
      "std            1.0           1.0            1.0               1.0   \n",
      "\n",
      "      INST_IS_LATE_MEAN  INCOME_PER_PERSON  DEBT_TO_INCOME_RATIO  \\\n",
      "mean                0.0               -0.0                   0.0   \n",
      "std                 1.0                1.0                   1.0   \n",
      "\n",
      "      CREDIT_TO_GOODS_RATIO  HAS_CHILDREN  TOTAL_DOCUMENTS_PROVIDED  \\\n",
      "mean                    0.0           0.0                       0.0   \n",
      "std                     1.0           1.0                       1.0   \n",
      "\n",
      "      EXT_SOURCE_MEAN  EXT_SOURCE_MAX  EXT_SOURCE_MIN  \\\n",
      "mean             -0.0             0.0             0.0   \n",
      "std               1.0             1.0             1.0   \n",
      "\n",
      "      NAME_CONTRACT_TYPE_Revolving_loans  CODE_GENDER_M  FLAG_OWN_CAR_Y  \\\n",
      "mean                                -0.0            0.0            -0.0   \n",
      "std                                  1.0            1.0             1.0   \n",
      "\n",
      "      FLAG_OWN_REALTY_Y  NAME_TYPE_SUITE_Family  \\\n",
      "mean                0.0                     0.0   \n",
      "std                 1.0                     1.0   \n",
      "\n",
      "      NAME_TYPE_SUITE_Spouse_partner  NAME_TYPE_SUITE_Unaccompanied  \\\n",
      "mean                             0.0                            0.0   \n",
      "std                              1.0                            1.0   \n",
      "\n",
      "      NAME_INCOME_TYPE_Commercial_associate  NAME_INCOME_TYPE_State_servant  \\\n",
      "mean                                    0.0                            -0.0   \n",
      "std                                     1.0                             1.0   \n",
      "\n",
      "      NAME_INCOME_TYPE_Working  NAME_EDUCATION_TYPE_Higher_education  \\\n",
      "mean                      -0.0                                   0.0   \n",
      "std                        1.0                                   1.0   \n",
      "\n",
      "      NAME_EDUCATION_TYPE_Incomplete_higher  \\\n",
      "mean                                    0.0   \n",
      "std                                     1.0   \n",
      "\n",
      "      NAME_EDUCATION_TYPE_Lower_secondary  \\\n",
      "mean                                  0.0   \n",
      "std                                   1.0   \n",
      "\n",
      "      NAME_EDUCATION_TYPE_Secondary_secondary_special  \\\n",
      "mean                                             -0.0   \n",
      "std                                               1.0   \n",
      "\n",
      "      NAME_FAMILY_STATUS_Married  NAME_FAMILY_STATUS_Separated  \\\n",
      "mean                        -0.0                           0.0   \n",
      "std                          1.0                           1.0   \n",
      "\n",
      "      NAME_FAMILY_STATUS_Single_not_married  NAME_FAMILY_STATUS_Widow  \\\n",
      "mean                                   -0.0                      -0.0   \n",
      "std                                     1.0                       1.0   \n",
      "\n",
      "      NAME_HOUSING_TYPE_House_apartment  \\\n",
      "mean                               -0.0   \n",
      "std                                 1.0   \n",
      "\n",
      "      NAME_HOUSING_TYPE_Municipal_apartment  \\\n",
      "mean                                    0.0   \n",
      "std                                     1.0   \n",
      "\n",
      "      NAME_HOUSING_TYPE_Rented_apartment  NAME_HOUSING_TYPE_With_parents  \\\n",
      "mean                                -0.0                             0.0   \n",
      "std                                  1.0                             1.0   \n",
      "\n",
      "      WEEKDAY_APPR_PROCESS_START_MONDAY  WEEKDAY_APPR_PROCESS_START_SATURDAY  \\\n",
      "mean                                0.0                                  0.0   \n",
      "std                                 1.0                                  1.0   \n",
      "\n",
      "      WEEKDAY_APPR_PROCESS_START_SUNDAY  WEEKDAY_APPR_PROCESS_START_THURSDAY  \\\n",
      "mean                               -0.0                                 -0.0   \n",
      "std                                 1.0                                  1.0   \n",
      "\n",
      "      WEEKDAY_APPR_PROCESS_START_TUESDAY  \\\n",
      "mean                                 0.0   \n",
      "std                                  1.0   \n",
      "\n",
      "      WEEKDAY_APPR_PROCESS_START_WEDNESDAY  \\\n",
      "mean                                  -0.0   \n",
      "std                                    1.0   \n",
      "\n",
      "      FONDKAPREMONT_MODE_org_spec_account  \\\n",
      "mean                                  0.0   \n",
      "std                                   1.0   \n",
      "\n",
      "      FONDKAPREMONT_MODE_reg_oper_account  \\\n",
      "mean                                  0.0   \n",
      "std                                   1.0   \n",
      "\n",
      "      FONDKAPREMONT_MODE_reg_oper_spec_account  WALLSMATERIAL_MODE_Panel  \\\n",
      "mean                                      -0.0                       0.0   \n",
      "std                                        1.0                       1.0   \n",
      "\n",
      "      WALLSMATERIAL_MODE_Stone_brick  WALLSMATERIAL_MODE_Wooden  \n",
      "mean                            -0.0                        0.0  \n",
      "std                              1.0                        1.0  \n",
      "\n",
      "[2 rows x 189 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use our new modular function\n",
    "X_train_scaled, X_test_scaled = scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"\\nScaled feature statistics:\")\n",
    "print(X_train_scaled.describe().loc[['mean', 'std']].round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîÄ Create Train-Validation Split\n",
    "\n",
    "**Important:** Use **STRATIFIED** split to preserve class distribution!\n",
    "\n",
    "We\\'ll create:\n",
    "- Training set (70%): For training models\n",
    "- Validation set (30%): For hyperparameter tuning and model selection\n",
    "- Test set: Already separate (for final evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:02:05.744464Z",
     "iopub.status.busy": "2025-12-06T12:02:05.743817Z",
     "iopub.status.idle": "2025-12-06T12:02:06.603441Z",
     "shell.execute_reply": "2025-12-06T12:02:06.600924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Validation Split:\n",
      "  Training: (215257, 189)\n",
      "  Validation: (92254, 189)\n",
      "  Test: (48744, 189)\n",
      "\n",
      "Class distribution verification:\n",
      "  Original: {0: 0.9192711805431351, 1: 0.08072881945686496}\n",
      "  Training: {0: 0.9192732408237595, 1: 0.08072675917624049}\n",
      "  Validation: {0: 0.9192663732737876, 1: 0.08073362672621241}\n",
      "\n",
      "[OK] Class distribution preserved!\n"
     ]
    }
   ],
   "source": [
    "# Stratified split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    test_size=0.3,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Train-Validation Split:\")\n",
    "print(f\"  Training: {X_train_split.shape}\")\n",
    "print(f\"  Validation: {X_val_split.shape}\")\n",
    "print(f\"  Test: {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(f\"  Original: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  Training: {y_train_split.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  Validation: {y_val_split.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "print(\"\\n[OK] Class distribution preserved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üíæ Save Processed Data\n",
    "\n",
    "Save the processed data so we can use it in the next notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:02:06.608986Z",
     "iopub.status.busy": "2025-12-06T12:02:06.608293Z",
     "iopub.status.idle": "2025-12-06T12:04:48.877853Z",
     "shell.execute_reply": "2025-12-06T12:04:48.875700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] All datasets saved!\n",
      "\n",
      "Saved files:\n",
      "  - X_train.csv: (215257, 189)\n",
      "  - X_val.csv: (92254, 189)\n",
      "  - X_test.csv: (48744, 189)\n",
      "  - y_train.csv, y_val.csv\n",
      "  - feature_names.csv: 189 features\n",
      "  - IDs for each split\n"
     ]
    }
   ],
   "source": [
    "# Create processed data directory\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "X_train_split.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "X_val_split.to_csv(processed_dir / 'X_val.csv', index=False)\n",
    "X_test_scaled.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "\n",
    "y_train_split.to_csv(processed_dir / 'y_train.csv', index=False, header=True)\n",
    "y_val_split.to_csv(processed_dir / 'y_val.csv', index=False, header=True)\n",
    "\n",
    "# Save feature names\n",
    "pd.DataFrame({'feature': X_train_split.columns}).to_csv(\n",
    "    processed_dir / 'feature_names.csv', index=False\n",
    ")\n",
    "\n",
    "# Save IDs\n",
    "train_df[['SK_ID_CURR']].iloc[X_train_split.index].to_csv(\n",
    "    processed_dir / 'train_ids.csv', index=False\n",
    ")\n",
    "train_df[['SK_ID_CURR']].iloc[X_val_split.index].to_csv(\n",
    "    processed_dir / 'val_ids.csv', index=False\n",
    ")\n",
    "test_df[['SK_ID_CURR']].to_csv(processed_dir / 'test_ids.csv', index=False)\n",
    "\n",
    "print(\"[OK] All datasets saved!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - X_train.csv: {X_train_split.shape}\")\n",
    "print(f\"  - X_val.csv: {X_val_split.shape}\")\n",
    "print(f\"  - X_test.csv: {X_test_scaled.shape}\")\n",
    "print(f\"  - y_train.csv, y_val.csv\")\n",
    "print(f\"  - feature_names.csv: {len(X_train_split.columns)} features\")\n",
    "print(f\"  - IDs for each split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Feature Engineering Summary\n",
    "\n",
    "### ‚úÖ What We Accomplished\n",
    "\n",
    "1. **Handled Missing Values**\n",
    "   - Dropped features with >70% missing\n",
    "   - Created missing indicators\n",
    "   - Imputed remaining values\n",
    "\n",
    "2. **Created Domain Features** (10+ new features)\n",
    "   - Age and employment features\n",
    "   - Income per person\n",
    "   - Debt-to-income ratio (KEY!)\n",
    "   - Credit utilization\n",
    "   - Payment burden ratio\n",
    "   - Family features\n",
    "   - Document counts\n",
    "   - External source aggregations\n",
    "\n",
    "3. **Encoded Categorical Variables**\n",
    "   - One-hot encoded low-cardinality features\n",
    "   - Handled high-cardinality features\n",
    "\n",
    "4. **Feature Selection**\n",
    "   - Removed low-variance features\n",
    "   - Removed highly correlated features\n",
    "   - Reduced feature count significantly\n",
    "\n",
    "5. **Scaled Features**\n",
    "   - StandardScaler (mean=0, std=1)\n",
    "   - Ready for modeling!\n",
    "\n",
    "6. **Created Train-Val Split**\n",
    "   - Stratified sampling\n",
    "   - 70% training, 30% validation\n",
    "\n",
    "### üéØ Key Features Created\n",
    "\n",
    "The most important features for credit scoring:\n",
    "1. **DEBT_TO_INCOME_RATIO** - How much debt vs income\n",
    "2. **ANNUITY_TO_INCOME_RATIO** - Can afford payments?\n",
    "3. **EMPLOYMENT_YEARS** - Stability indicator\n",
    "4. **AGE_YEARS** - Risk correlates with age\n",
    "5. **INCOME_PER_PERSON** - Family financial situation\n",
    "6. **CREDIT_UTILIZATION** - Credit usage patterns\n",
    "\n",
    "### üìä Final Dataset\n",
    "\n",
    "- **Features:** ~100-150 (after selection and engineering)\n",
    "- **Training samples:** ~215,000\n",
    "- **Validation samples:** ~92,000\n",
    "- **Test samples:** ~48,000\n",
    "- **Class balance:** ~8% positive class (defaults)\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "In the next notebook ([03_baseline_models.ipynb](03_baseline_models.ipynb)), we will:\n",
    "\n",
    "1. **Train Multiple Baseline Models**\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "   - LightGBM\n",
    "\n",
    "2. **Set Up MLflow Tracking**\n",
    "   - Log all experiments\n",
    "   - Compare models\n",
    "   - Save artifacts\n",
    "\n",
    "3. **Evaluate Using Appropriate Metrics**\n",
    "   - ROC-AUC\n",
    "   - Precision-Recall AUC\n",
    "   - F1-Score\n",
    "   - Confusion Matrix\n",
    "\n",
    "4. **Select Best Baseline Model**\n",
    "   - Compare performance\n",
    "   - Consider interpretability\n",
    "   - Choose for optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work on feature engineering! üéâ**\n",
    "\n",
    "Your data is now ready for modeling. Remember: good features are often more important than complex models!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoring-model-py3.13 (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
