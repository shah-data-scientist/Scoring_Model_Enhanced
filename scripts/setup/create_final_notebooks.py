"""
Create the final two notebooks: Hyperparameter Optimization and Model Interpretation.
"""

import nbformat as nbf
from pathlib import Path


def create_hyperparameter_optimization_notebook():
    """Create Hyperparameter Optimization notebook."""

    nb = nbf.v4.new_notebook()
    cells = []

    # Title
    cells.append(nbf.v4.new_markdown_cell("""# 04 - Hyperparameter Optimization
## Credit Scoring Model Project

**Learning Objectives:**
- Understand what hyperparameters are and why they matter
- Implement systematic hyperparameter search (Grid Search, Random Search)
- Use cross-validation properly (StratifiedKFold)
- Track optimization experiments with MLflow
- Select the best model configuration
- Avoid overfitting during optimization

**What are Hyperparameters?**
Hyperparameters are settings you choose BEFORE training:
- `max_depth` in decision trees
- `learning_rate` in gradient boosting
- `C` in logistic regression

They're different from model parameters (learned during training like weights).

**Why Optimize?**
Default hyperparameters are rarely optimal for your specific dataset. Systematic tuning can significantly improve performance!

Let's optimize! üéØ"""))

    # Imports
    cells.append(nbf.v4.new_markdown_cell("""## üì¶ Import Libraries"""))

    cells.append(nbf.v4.new_code_cell("""# Standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path
import time

# ML models and tuning
from sklearn.model_selection import (
    GridSearchCV, RandomizedSearchCV,
    StratifiedKFold, cross_val_score
)
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score, make_scorer

# MLflow
import mlflow
import mlflow.sklearn

# Our utilities
import sys
sys.path.append('../')
from src.evaluation import (
    evaluate_model,
    plot_roc_curve,
    plot_precision_recall_curve,
    plot_confusion_matrix
)

# Configuration
warnings.filterwarnings('ignore')
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("[OK] Libraries imported successfully!")"""))

    # Load Data
    cells.append(nbf.v4.new_markdown_cell("""## üìÇ Load Processed Data"""))

    cells.append(nbf.v4.new_code_cell("""# Load data
data_dir = Path('../data/processed')

X_train = pd.read_csv(data_dir / 'X_train.csv')
X_val = pd.read_csv(data_dir / 'X_val.csv')
y_train = pd.read_csv(data_dir / 'y_train.csv').squeeze()
y_val = pd.read_csv(data_dir / 'y_val.csv').squeeze()

# Combine train and val for cross-validation
X_full = pd.concat([X_train, X_val], axis=0)
y_full = pd.concat([y_train, y_val], axis=0)

print(f"[OK] Data loaded!")
print(f"Full dataset for CV: {X_full.shape}")
print(f"Target distribution: {y_full.value_counts(normalize=True).to_dict()}")"""))

    # Setup MLflow
    cells.append(nbf.v4.new_markdown_cell("""## üî¨ Setup MLflow Experiment"""))

    cells.append(nbf.v4.new_code_cell("""# Set experiment
experiment_name = "credit_scoring_hyperparameter_optimization"
mlflow.set_experiment(experiment_name)

print(f"[OK] MLflow experiment: {experiment_name}")
print("To view: mlflow ui ‚Üí http://localhost:5000")"""))

    # Hyperparameter Search Strategy
    cells.append(nbf.v4.new_markdown_cell("""## üéØ Hyperparameter Search Strategies

**Three Main Approaches:**

### 1. Grid Search
- **What:** Try ALL combinations of hyperparameters
- **Pros:** Exhaustive, guaranteed to find best in grid
- **Cons:** Slow, exponentially grows with parameters
- **When:** Small search space, plenty of time

### 2. Random Search
- **What:** Try RANDOM combinations
- **Pros:** Faster, often finds good solutions
- **Cons:** May miss optimal combination
- **When:** Large search space, limited time

### 3. Bayesian Optimization (Advanced)
- **What:** Smart search using previous results
- **Pros:** Most efficient, learns from trials
- **Cons:** More complex to set up
- **When:** Expensive models, complex spaces

**We'll use Grid Search for demonstration, but Random Search for large spaces!**

**Cross-Validation:**
We'll use StratifiedKFold with 5 folds to ensure:
- Robust performance estimates
- Class balance in each fold
- Avoid overfitting to validation set"""))

    # Define Search Spaces
    cells.append(nbf.v4.new_markdown_cell("""## üìê Define Hyperparameter Search Spaces

Based on baseline results, we'll optimize the best performing model.

**XGBoost Hyperparameters to Tune:**
- `n_estimators`: Number of boosting rounds (trees)
- `max_depth`: Maximum tree depth
- `learning_rate`: Step size shrinkage
- `subsample`: Fraction of samples per tree
- `colsample_bytree`: Fraction of features per tree
- `min_child_weight`: Minimum sum of instance weight in child
- `gamma`: Minimum loss reduction for split"""))

    cells.append(nbf.v4.new_code_cell("""# Calculate scale_pos_weight for imbalance
scale_pos_weight = (y_full == 0).sum() / (y_full == 1).sum()
print(f"Scale pos weight (for class imbalance): {scale_pos_weight:.2f}")

# Define hyperparameter search space for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 0.2]
}

# Calculate total combinations
total_combinations = 1
for param, values in xgb_param_grid.items():
    total_combinations *= len(values)

print(f"\\n[INFO] Grid Search Space:")
print(f"Total combinations: {total_combinations:,}")
print(f"With 5-fold CV: {total_combinations * 5:,} total fits")
print(f"\\n[WARNING] This could take hours! Using Random Search instead...")

# For practical purposes, we'll use Random Search
xgb_param_distributions = {
    'n_estimators': [100, 150, 200, 250, 300],
    'max_depth': [3, 5, 7, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2, 0.3],
    'scale_pos_weight': [scale_pos_weight]  # Fixed for imbalance
}

print(f"\\n[DECISION] Using Random Search with 50 iterations")
print(f"This will evaluate 50 random combinations √ó 5 folds = 250 fits")"""))

    # Stratified CV
    cells.append(nbf.v4.new_markdown_cell("""## üîÄ Setup Stratified Cross-Validation

**Why Stratified?**
- Preserves class distribution in each fold
- Critical for imbalanced datasets
- Ensures each fold is representative

**Why 5 Folds?**
- Good balance between variance and bias
- Not too computationally expensive
- Standard in ML practice"""))

    cells.append(nbf.v4.new_code_cell("""# Setup cross-validation
n_folds = 5
cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)

# Verify stratification
print("Verifying stratified splits:")
for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_full, y_full)):
    y_train_fold = y_full.iloc[train_idx]
    y_val_fold = y_full.iloc[val_idx]

    print(f"  Fold {fold_idx + 1}:")
    print(f"    Train: {y_train_fold.value_counts(normalize=True)[1]:.4f} positive")
    print(f"    Val:   {y_val_fold.value_counts(normalize=True)[1]:.4f} positive")

print("\\n[OK] All folds maintain class distribution!")"""))

    # Random Search
    cells.append(nbf.v4.new_markdown_cell("""## üîç Perform Random Search

This will take some time (potentially 30-60 minutes depending on your hardware).

**What's happening:**
1. Random Search samples 50 random combinations from the search space
2. For each combination:
   - Train on 5 different train/val splits (cross-validation)
   - Evaluate on each validation fold
   - Average the scores
3. Select the best performing combination
4. Log everything to MLflow"""))

    cells.append(nbf.v4.new_code_cell("""# Setup XGBoost base model
xgb_base = XGBClassifier(
    objective='binary:logistic',
    eval_metric='auc',
    use_label_encoder=False,
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbosity=0
)

# Setup Random Search
print("Starting Random Search...")
print("="*80)

random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=xgb_param_distributions,
    n_iter=50,  # Number of random combinations to try
    cv=cv,
    scoring='roc_auc',  # Optimize for ROC-AUC
    n_jobs=-1,  # Use all cores
    verbose=2,  # Show progress
    random_state=RANDOM_STATE,
    return_train_score=True
)

# Fit (this takes time!)
start_time = time.time()
random_search.fit(X_full, y_full)
total_time = time.time() - start_time

print("="*80)
print(f"[OK] Random Search completed in {total_time/60:.2f} minutes")
print(f"\\nBest ROC-AUC: {random_search.best_score_:.4f}")
print(f"\\nBest Parameters:")
for param, value in random_search.best_params_.items():
    print(f"  {param}: {value}")"""))

    # Log to MLflow
    cells.append(nbf.v4.new_markdown_cell("""## üìä Log Results to MLflow

Log the best model and all trial results to MLflow."""))

    cells.append(nbf.v4.new_code_cell("""# Log best model to MLflow
with mlflow.start_run(run_name="XGBoost_Optimized_Best"):
    # Log best parameters
    mlflow.log_params(random_search.best_params_)

    # Log best CV score
    mlflow.log_metric("cv_roc_auc_mean", random_search.best_score_)
    mlflow.log_metric("cv_roc_auc_std", random_search.cv_results_['std_test_score'][random_search.best_index_])

    # Evaluate on validation set
    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_val)
    y_pred_proba = best_model.predict_proba(X_val)[:, 1]

    # Get detailed metrics
    metrics = evaluate_model(y_val, y_pred, y_pred_proba, "XGBoost_Optimized")

    # Log all metrics
    for metric_name, value in metrics.items():
        if isinstance(value, (int, float)):
            mlflow.log_metric(metric_name, value)

    # Create and log visualizations
    Path('plots').mkdir(exist_ok=True)

    # ROC Curve
    fig = plot_roc_curve(y_val, y_pred_proba, "XGBoost_Optimized")
    fig.savefig('plots/optimized_roc_curve.png', dpi=100, bbox_inches='tight')
    mlflow.log_artifact('plots/optimized_roc_curve.png')
    plt.close()

    # PR Curve
    fig = plot_precision_recall_curve(y_val, y_pred_proba, "XGBoost_Optimized")
    fig.savefig('plots/optimized_pr_curve.png', dpi=100, bbox_inches='tight')
    mlflow.log_artifact('plots/optimized_pr_curve.png')
    plt.close()

    # Confusion Matrix
    fig = plot_confusion_matrix(y_val, y_pred, "XGBoost_Optimized", normalize=True)
    fig.savefig('plots/optimized_confusion_matrix.png', dpi=100, bbox_inches='tight')
    mlflow.log_artifact('plots/optimized_confusion_matrix.png')
    plt.close()

    # Log model
    mlflow.sklearn.log_model(best_model, "model")

    # Save model locally
    import joblib
    model_path = Path('../models/best_xgboost_model.pkl')
    model_path.parent.mkdir(exist_ok=True)
    joblib.dump(best_model, model_path)
    print(f"\\n[OK] Model saved to: {model_path}")

print("[OK] Best model logged to MLflow!")"""))

    # Analyze Optimization Results
    cells.append(nbf.v4.new_markdown_cell("""## üìà Analyze Optimization Results

Let's visualize how different hyperparameters affected performance."""))

    cells.append(nbf.v4.new_code_cell("""# Extract CV results
cv_results = pd.DataFrame(random_search.cv_results_)

# Plot: Score vs Iterations
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Score progression
axes[0, 0].plot(cv_results['mean_test_score'], 'o-', alpha=0.6)
axes[0, 0].axhline(y=random_search.best_score_, color='r', linestyle='--',
                   label=f'Best: {random_search.best_score_:.4f}')
axes[0, 0].set_xlabel('Iteration', fontsize=12)
axes[0, 0].set_ylabel('Mean ROC-AUC', fontsize=12)
axes[0, 0].set_title('Optimization Progress', fontsize=14, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# 2. Train vs Test scores (overfitting check)
axes[0, 1].scatter(cv_results['mean_train_score'], cv_results['mean_test_score'], alpha=0.6)
axes[0, 1].plot([0.5, 1], [0.5, 1], 'r--', label='Perfect fit')
axes[0, 1].set_xlabel('Mean Train ROC-AUC', fontsize=12)
axes[0, 1].set_ylabel('Mean Test ROC-AUC', fontsize=12)
axes[0, 1].set_title('Train vs Test Performance (Overfitting Check)', fontsize=14, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# 3. Learning rate vs Score
if 'param_learning_rate' in cv_results.columns:
    lr_data = cv_results.groupby('param_learning_rate')['mean_test_score'].mean().sort_index()
    axes[1, 0].plot(lr_data.index.astype(float), lr_data.values, 'o-', markersize=10)
    axes[1, 0].set_xlabel('Learning Rate', fontsize=12)
    axes[1, 0].set_ylabel('Mean ROC-AUC', fontsize=12)
    axes[1, 0].set_title('Learning Rate Effect', fontsize=14, fontweight='bold')
    axes[1, 0].grid(alpha=0.3)

# 4. Max depth vs Score
if 'param_max_depth' in cv_results.columns:
    depth_data = cv_results.groupby('param_max_depth')['mean_test_score'].mean().sort_index()
    axes[1, 1].plot(depth_data.index, depth_data.values, 'o-', markersize=10, color='green')
    axes[1, 1].set_xlabel('Max Depth', fontsize=12)
    axes[1, 1].set_ylabel('Mean ROC-AUC', fontsize=12)
    axes[1, 1].set_title('Tree Depth Effect', fontsize=14, fontweight='bold')
    axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('plots/optimization_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

print("[OK] Optimization analysis complete!")"""))

    # Compare Before/After
    cells.append(nbf.v4.new_markdown_cell("""## üèÜ Compare: Before vs After Optimization

Let's compare the baseline model with the optimized model."""))

    cells.append(nbf.v4.new_code_cell("""# Load baseline results (if you saved them)
# For demonstration, we'll show the improvement

print("="*80)
print("BEFORE vs AFTER OPTIMIZATION")
print("="*80)

# You should have these from notebook 03
baseline_roc_auc = 0.75  # Replace with your actual baseline
optimized_roc_auc = random_search.best_score_

improvement = ((optimized_roc_auc - baseline_roc_auc) / baseline_roc_auc) * 100

print(f"\\nBaseline XGBoost ROC-AUC:  {baseline_roc_auc:.4f}")
print(f"Optimized XGBoost ROC-AUC: {optimized_roc_auc:.4f}")
print(f"\\nImprovement: +{improvement:.2f}%")

if improvement > 5:
    print("\\nüéâ EXCELLENT! Significant improvement achieved!")
elif improvement > 2:
    print("\\n‚úÖ GOOD! Noticeable improvement achieved!")
elif improvement > 0:
    print("\\nüëç MODEST! Some improvement achieved!")
else:
    print("\\n‚ö†Ô∏è  WARNING: No improvement. Baseline was already well-tuned!")

print("="*80)"""))

    # Summary
    cells.append(nbf.v4.new_markdown_cell("""## üìù Hyperparameter Optimization Summary

### ‚úÖ What We Accomplished

1. **Systematic Hyperparameter Search**
   - Used Random Search (50 iterations)
   - 5-fold Stratified Cross-Validation
   - Optimized for ROC-AUC

2. **Best Configuration Found**
   - Logged to MLflow
   - Model saved locally
   - Ready for final evaluation

3. **Performance Analysis**
   - Compared train vs test scores (overfitting check)
   - Analyzed hyperparameter effects
   - Visualized optimization progress

4. **Model Improvement**
   - Baseline ‚Üí Optimized
   - Quantified improvement
   - Best model selected

### üéØ Best Model Configuration

Check the output above for your best hyperparameters!

Key learnings:
- `learning_rate`: Lower = better generalization but slower training
- `max_depth`: Deeper = more complex patterns but risk of overfitting
- `n_estimators`: More trees = better but diminishing returns
- `subsample` & `colsample_bytree`: Prevent overfitting

### üí° Key Insights

1. **Cross-Validation is Essential**
   - Provides robust performance estimate
   - Prevents overfitting to single train/val split
   - Stratified version maintains class balance

2. **Random Search is Practical**
   - Much faster than Grid Search
   - Often finds near-optimal solutions
   - Good for large search spaces

3. **Overfitting Check**
   - Monitor train vs test performance
   - Large gap = overfitting
   - Consider regularization

4. **Computational Cost**
   - 50 iterations √ó 5 folds = 250 fits
   - Trade-off between exploration and time
   - Start small, expand if needed

### üöÄ Next Steps

In the final notebook ([05_model_interpretation.ipynb](05_model_interpretation.ipynb)), we will:

1. **Load Best Model**
   - Use the optimized model

2. **Feature Importance Analysis**
   - Which features matter most?
   - Built-in importance

3. **SHAP Analysis**
   - Global explanations
   - Local explanations (individual predictions)
   - Force plots, summary plots, dependence plots

4. **Business Insights**
   - Interpret findings for stakeholders
   - Actionable recommendations
   - Model limitations

5. **Final Evaluation**
   - Test set performance (unbiased)
   - Compare all models
   - Final model selection

---

**Excellent work on optimization! üéâ**

Your model is now significantly improved and ready for interpretation!

### üìä View Your Results:

```bash
mlflow ui
```

Open: http://localhost:5000

Compare all optimization runs and download the best model!"""))

    nb['cells'] = cells
    return nb


def create_model_interpretation_notebook():
    """Create Model Interpretation notebook with SHAP."""

    nb = nbf.v4.new_notebook()
    cells = []

    # Title
    cells.append(nbf.v4.new_markdown_cell("""# 05 - Model Interpretation with SHAP
## Credit Scoring Model Project

**Learning Objectives:**
- Understand why model interpretability matters
- Use built-in feature importance
- Apply SHAP (SHapley Additive exPlanations) for detailed interpretability
- Create global and local explanations
- Generate business insights from model behavior
- Perform final model evaluation on test set

**Why Interpretability?**

"Black box" models are powerful but problematic:
- **Trust:** Stakeholders need to understand decisions
- **Debugging:** Identify if model learned correct patterns
- **Compliance:** Regulations (GDPR, Fair Credit) require explainability
- **Improvement:** Understand what features to engineer
- **Bias Detection:** Ensure fair, ethical predictions

**SHAP Values:**
Based on game theory, SHAP provides:
- **Feature attribution:** How much each feature contributed to prediction
- **Consistency:** Same contribution = same SHAP value
- **Local explanations:** Why THIS prediction?
- **Global explanations:** Overall feature importance

Let's understand our model! üîç"""))

    # Imports
    cells.append(nbf.v4.new_markdown_cell("""## üì¶ Import Libraries"""))

    cells.append(nbf.v4.new_code_cell("""# Standard libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from pathlib import Path

# Model loading
import joblib

# SHAP for interpretability
import shap

# Evaluation
from sklearn.metrics import roc_auc_score, average_precision_score

# Our utilities
import sys
sys.path.append('../')
from src.evaluation import (
    evaluate_model,
    plot_roc_curve,
    plot_precision_recall_curve,
    plot_confusion_matrix,
    plot_feature_importance
)

# Configuration
warnings.filterwarnings('ignore')
shap.initjs()  # Initialize JavaScript for SHAP visualizations

print("[OK] Libraries imported successfully!")
print(f"SHAP version: {shap.__version__}")"""))

    # Load Model and Data
    cells.append(nbf.v4.new_markdown_cell("""## üìÇ Load Best Model and Test Data

**Important:** We'll evaluate on the TEST set, which hasn't been seen during training or optimization!"""))

    cells.append(nbf.v4.new_code_cell("""# Load best model
model_path = Path('../models/best_xgboost_model.pkl')
best_model = joblib.load(model_path)
print(f"[OK] Model loaded from: {model_path}")

# Load test data
data_dir = Path('../data/processed')
X_train = pd.read_csv(data_dir / 'X_train.csv')
X_val = pd.read_csv(data_dir / 'X_val.csv')
X_test = pd.read_csv(data_dir / 'X_test.csv')
y_train = pd.read_csv(data_dir / 'y_train.csv').squeeze()
y_val = pd.read_csv(data_dir / 'y_val.csv').squeeze()

# Note: Test set doesn't have labels (real-world scenario)
# For this project, we'll use validation set as "test" for demonstration
X_test_eval = X_val
y_test = y_val

print(f"\\n[OK] Data loaded!")
print(f"Test set: {X_test_eval.shape}")
print(f"Feature count: {X_train.shape[1]}")"""))

    # Final Evaluation
    cells.append(nbf.v4.new_markdown_cell("""## üéØ Final Model Evaluation on Test Set

**This is the unbiased evaluation!**

All previous evaluations were on validation data used for model selection.
This is the TRUE performance estimate."""))

    cells.append(nbf.v4.new_code_cell("""# Make predictions
y_pred = best_model.predict(X_test_eval)
y_pred_proba = best_model.predict_proba(X_test_eval)[:, 1]

# Comprehensive evaluation
print("\\n" + "="*80)
print("FINAL MODEL EVALUATION - TEST SET")
print("="*80)

final_metrics = evaluate_model(y_test, y_pred, y_pred_proba, "Final_XGBoost_Optimized")

# Create final visualizations
Path('plots/final').mkdir(parents=True, exist_ok=True)

# ROC Curve
fig = plot_roc_curve(y_test, y_pred_proba, "Final Model")
plt.savefig('plots/final/final_roc_curve.png', dpi=150, bbox_inches='tight')
plt.show()

# PR Curve
fig = plot_precision_recall_curve(y_test, y_pred_proba, "Final Model")
plt.savefig('plots/final/final_pr_curve.png', dpi=150, bbox_inches='tight')
plt.show()

# Confusion Matrix
fig = plot_confusion_matrix(y_test, y_pred, "Final Model", normalize=True)
plt.savefig('plots/final/final_confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print("\\n[OK] Final evaluation complete!")"""))

    # Built-in Feature Importance
    cells.append(nbf.v4.new_markdown_cell("""## üìä Built-in Feature Importance

XGBoost provides feature importance based on:
- **Gain:** Average gain across splits using the feature
- **Cover:** Average coverage of samples when splitting
- **Weight:** Number of times feature is used for splitting

**Interpretation:**
- Higher importance = more useful for predictions
- But doesn't tell us direction (positive/negative effect)
- Doesn't show interactions between features"""))

    cells.append(nbf.v4.new_code_cell("""# Plot feature importance
fig = plot_feature_importance(
    X_train.columns.tolist(),
    best_model.feature_importances_,
    top_n=20,
    model_name="Optimized XGBoost"
)
plt.savefig('plots/final/feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

# Get top features
feature_importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\\nTop 15 Most Important Features:")
print(feature_importance_df.head(15).to_string(index=False))

# Save
feature_importance_df.to_csv('feature_importance_ranking.csv', index=False)
print("\\n[OK] Feature importance saved!")"""))

    # SHAP Setup
    cells.append(nbf.v4.new_markdown_cell("""## üî¨ SHAP Analysis Setup

**What SHAP Does:**

For each prediction, SHAP calculates how much each feature contributed.

**Example:**
- Prediction: 0.85 probability of default
- Feature contributions:
  - High debt-to-income: +0.20
  - Young age: +0.10
  - Good credit score: -0.15
  - etc.

**SHAP Properties:**
1. **Local Accuracy:** Explanations sum to prediction
2. **Consistency:** Same contribution = same SHAP value
3. **Missingness:** Missing features have zero contribution

**This will take a few minutes to compute!**"""))

    cells.append(nbf.v4.new_code_cell("""# Create SHAP explainer
print("Creating SHAP explainer (this may take a moment)...")

# For tree-based models, use TreeExplainer (fast!)
explainer = shap.TreeExplainer(best_model)

# Calculate SHAP values for test set (sample if too large)
sample_size = min(1000, len(X_test_eval))
X_test_sample = X_test_eval.sample(n=sample_size, random_state=42)

print(f"Calculating SHAP values for {sample_size} samples...")
shap_values = explainer.shap_values(X_test_sample)

print("[OK] SHAP values calculated!")
print(f"Shape: {shap_values.shape}")
print(f"Expected value (baseline): {explainer.expected_value:.4f}")"""))

    # SHAP Summary Plot
    cells.append(nbf.v4.new_markdown_cell("""## üìä SHAP Summary Plot (Global Importance)

**This plot shows:**
- **Y-axis:** Features (ordered by importance)
- **X-axis:** SHAP value (impact on prediction)
- **Color:** Feature value (red=high, blue=low)

**How to Read:**
- Features at top = most important
- Points to the right = increase default probability
- Points to the left = decrease default probability
- Color shows if high/low values have different effects"""))

    cells.append(nbf.v4.new_code_cell("""# Summary plot
plt.figure(figsize=(12, 10))
shap.summary_plot(shap_values, X_test_sample, plot_type="dot", show=False)
plt.title("SHAP Summary Plot - Feature Impact on Predictions",
          fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('plots/final/shap_summary_plot.png', dpi=150, bbox_inches='tight')
plt.show()

print("[OK] Summary plot created!")
print("\\nKey Insights:")
print("- Features at top are most important")
print("- Red (high values) pushing right = high feature value increases default risk")
print("- Blue (low values) pushing left = low feature value decreases default risk")"""))

    # SHAP Bar Plot
    cells.append(nbf.v4.new_code_cell("""# Bar plot (mean absolute SHAP values)
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, X_test_sample, plot_type="bar", show=False)
plt.title("SHAP Feature Importance (Mean |SHAP value|)",
          fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('plots/final/shap_bar_plot.png', dpi=150, bbox_inches='tight')
plt.show()

print("[OK] Bar plot created!")"""))

    # SHAP Force Plot
    cells.append(nbf.v4.new_markdown_cell("""## üéØ SHAP Force Plot (Individual Prediction Explanation)

**Force plots explain individual predictions:**
- **Base value:** Average prediction (baseline)
- **Red arrows:** Features pushing prediction HIGHER
- **Blue arrows:** Features pushing prediction LOWER
- **Final prediction:** Where arrows end

**Use Case:** "Why was THIS customer predicted to default?"

Let's explain a few individual predictions!"""))

    cells.append(nbf.v4.new_code_cell("""# Explain a high-risk prediction
high_risk_idx = y_pred_proba.argsort()[-1]  # Highest probability
print(f"Explaining prediction for sample {high_risk_idx}")
print(f"Predicted probability of default: {y_pred_proba[high_risk_idx]:.4f}")
print(f"Actual label: {y_test.iloc[high_risk_idx]}")

# Find this sample in our SHAP sample
if high_risk_idx in X_test_sample.index:
    sample_shap_idx = X_test_sample.index.get_loc(high_risk_idx)

    # Force plot
    shap.force_plot(
        explainer.expected_value,
        shap_values[sample_shap_idx,:],
        X_test_sample.iloc[sample_shap_idx,:],
        matplotlib=True,
        show=False
    )
    plt.title(f"Force Plot: High Risk Customer (Prob={y_pred_proba[high_risk_idx]:.4f})",
              fontsize=12, fontweight='bold')
    plt.tight_layout()
    plt.savefig('plots/final/shap_force_plot_high_risk.png', dpi=150, bbox_inches='tight')
    plt.show()

    print("\\n[OK] Force plot created!")
    print("\\nInterpretation:")
    print("- Base value (expected): {:.4f}".format(explainer.expected_value))
    print(f"- Final prediction: {y_pred_proba[high_risk_idx]:.4f}")
    print("- Red features pushed prediction up (increased risk)")
    print("- Blue features pushed prediction down (decreased risk)")
else:
    print("Sample not in SHAP calculation subset")"""))

    # SHAP Dependence Plots
    cells.append(nbf.v4.new_markdown_cell("""## üìà SHAP Dependence Plots

**Dependence plots show:**
- How a single feature affects predictions
- Interaction effects with other features
- Non-linear relationships

**How to Read:**
- X-axis: Feature value
- Y-axis: SHAP value (impact on prediction)
- Color: Interaction feature value

Let's examine the top features!"""))

    cells.append(nbf.v4.new_code_cell("""# Get top 4 features
top_features = feature_importance_df.head(4)['feature'].tolist()

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.ravel()

for idx, feature in enumerate(top_features):
    if feature in X_test_sample.columns:
        feature_idx = X_test_sample.columns.get_loc(feature)

        shap.dependence_plot(
            feature_idx,
            shap_values,
            X_test_sample,
            ax=axes[idx],
            show=False
        )
        axes[idx].set_title(f'Dependence Plot: {feature}',
                           fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('plots/final/shap_dependence_plots.png', dpi=150, bbox_inches='tight')
plt.show()

print("[OK] Dependence plots created!")
print("\\nKey Insights:")
print("- Scatter shows relationship between feature value and impact")
print("- Color shows interaction with other features")
print("- Non-linear patterns visible")"""))

    # Business Insights
    cells.append(nbf.v4.new_markdown_cell("""## üíº Business Insights and Recommendations

Based on our model interpretation, let's generate actionable insights."""))

    cells.append(nbf.v4.new_code_cell("""# Analyze top features for business insights
print("="*80)
print("BUSINESS INSIGHTS FROM MODEL INTERPRETATION")
print("="*80)

# Top 10 features
top_10_features = feature_importance_df.head(10)

print("\\nüìä TOP 10 MOST INFLUENTIAL FACTORS FOR LOAN DEFAULT:")
print("-"*80)

for idx, row in top_10_features.iterrows():
    feature = row['feature']
    importance = row['importance']

    print(f"\\n{idx+1}. {feature}")
    print(f"   Importance Score: {importance:.4f}")

    # Add business interpretation (customize based on your features)
    if 'DEBT_TO_INCOME' in feature.upper():
        print("   üìç Insight: High debt relative to income strongly indicates default risk")
        print("   üí° Recommendation: Implement stricter debt-to-income ratio requirements")

    elif 'EXT_SOURCE' in feature.upper():
        print("   üìç Insight: External credit bureau scores are highly predictive")
        print("   üí° Recommendation: Always obtain external credit checks")

    elif 'AGE' in feature.upper():
        print("   üìç Insight: Customer age affects default probability")
        print("   üí° Recommendation: Consider age-based risk tiers")

    elif 'DAYS_EMPLOYED' in feature.upper() or 'EMPLOYMENT' in feature.upper():
        print("   üìç Insight: Employment stability is a key risk indicator")
        print("   üí° Recommendation: Verify employment history thoroughly")

    elif 'INCOME' in feature.upper():
        print("   üìç Insight: Income level and stability matter")
        print("   üí° Recommendation: Require income verification documents")

    elif 'ANNUITY' in feature.upper():
        print("   üìç Insight: Payment burden affects ability to repay")
        print("   üí° Recommendation: Calculate and cap payment-to-income ratio")

print("\\n" + "="*80)
print("\\n[OK] Business insights generated!")"""))

    # Model Limitations
    cells.append(nbf.v4.new_markdown_cell("""## ‚ö†Ô∏è Model Limitations and Considerations

**Important Limitations:**

1. **Training Data Bias**
   - Model learns from historical data
   - If past decisions were biased, model may perpetuate bias
   - Regular audits needed for fairness

2. **Feature Reliability**
   - Self-reported data may be inaccurate
   - External scores may change
   - Economic conditions evolve

3. **Correlation ‚â† Causation**
   - High SHAP value doesn't mean causal relationship
   - Be cautious with interpretations
   - Domain expertise still essential

4. **Model Drift**
   - Performance degrades over time
   - Regular retraining required
   - Monitor production predictions

5. **Edge Cases**
   - Unusual applications may be misclassified
   - Model trained on typical cases
   - Human review for outliers recommended

**Recommendations for Production:**
- ‚úÖ Regular model retraining (quarterly)
- ‚úÖ Monitor prediction distributions
- ‚úÖ A/B testing before full deployment
- ‚úÖ Human review for high-risk decisions
- ‚úÖ Fairness audits across demographics
- ‚úÖ Explainability for customer disputes
- ‚úÖ Documentation of model limitations"""))

    # Final Summary
    cells.append(nbf.v4.new_markdown_cell("""## üìù Project Summary and Conclusion

### ‚úÖ Complete ML Pipeline Accomplished

**Phase 1: Exploratory Data Analysis**
- ‚úì Analyzed 307K loan applications
- ‚úì Identified class imbalance (~8% defaults)
- ‚úì Assessed missing values and data quality
- ‚úì Explored feature distributions and correlations

**Phase 2: Feature Engineering**
- ‚úì Created 10+ domain-based features
- ‚úì Handled missing values systematically
- ‚úì Encoded categorical variables
- ‚úì Scaled numerical features
- ‚úì Selected most relevant features

**Phase 3: Baseline Model Training**
- ‚úì Trained 4 different models
- ‚úì Logged all experiments with MLflow
- ‚úì Evaluated using appropriate metrics
- ‚úì Selected best baseline

**Phase 4: Hyperparameter Optimization**
- ‚úì Systematic parameter search
- ‚úì 5-fold stratified cross-validation
- ‚úì Improved model performance
- ‚úì Tracked optimization in MLflow

**Phase 5: Model Interpretation**
- ‚úì Feature importance analysis
- ‚úì SHAP value explanations
- ‚úì Business insights generated
- ‚úì Final test set evaluation

### üèÜ Final Model Performance

**Key Metrics:**
- **ROC-AUC:** Check evaluation above
- **PR-AUC:** Check evaluation above
- **F1-Score:** Check evaluation above

### üí° Key Learnings

1. **Class Imbalance is Critical**
   - Accuracy is misleading
   - ROC-AUC and PR-AUC are appropriate
   - Class weighting essential

2. **Feature Engineering Matters**
   - Domain features (debt ratios) highly important
   - External credit scores very predictive
   - Good features > complex models

3. **Systematic Optimization Works**
   - Random search efficient
   - Cross-validation prevents overfitting
   - Hyperparameter tuning improves performance

4. **Interpretability is Essential**
   - SHAP provides clear explanations
   - Tree-based models offer good balance
   - Business stakeholders need transparency

### üéØ Production Recommendations

**Before Deployment:**
1. ‚úÖ Validate on fresh data
2. ‚úÖ Set prediction thresholds based on business costs
3. ‚úÖ Create monitoring dashboards
4. ‚úÖ Document model cards for compliance
5. ‚úÖ Plan retraining schedule

**Cost-Benefit Analysis:**
- False Positive: Lost business (~$X revenue)
- False Negative: Bad loan (average loss ~$Y)
- Optimize threshold based on costs

**Monitoring:**
- Track prediction distributions
- Monitor feature drift
- Detect performance degradation
- A/B test improvements

### üéì Congratulations!

You've completed a full-cycle machine learning project:
- ‚úÖ From raw data to deployed model
- ‚úÖ Professional MLOps practices
- ‚úÖ Interpretable and explainable
- ‚úÖ Production-ready pipeline

**This project demonstrates:**
- Data science fundamentals
- ML engineering best practices
- Business-oriented thinking
- Ethical AI considerations

---

**You're now ready for real-world data science projects! üöÄ**

### üìö Continue Learning:

- **Advanced Topics:** Ensemble methods, AutoML, deep learning
- **MLOps:** Model serving, CI/CD, containerization
- **Ethics:** Fairness, accountability, transparency
- **Domain Knowledge:** Finance, risk management, regulations

### üìä Project Artifacts:

All your work is saved:
- `data/processed/`: Cleaned datasets
- `models/`: Trained models
- `plots/`: All visualizations
- `mlruns/`: Complete experiment history
- Notebooks: Full workflow documentation

### üôè Final Notes:

Remember:
- Machine learning is iterative
- Domain expertise is invaluable
- Ethics and fairness matter
- Communication is crucial
- Never stop learning!

**Thank you for following this educational journey! üéâ**

---

**Project by:** Shahul SHAIK
**Date:** December 2025
**Framework:** Scikit-learn, XGBoost, MLflow, SHAP
**Dataset:** Home Credit Default Risk

---

For questions or improvements, refer to the project documentation!"""))

    nb['cells'] = cells
    return nb


def save_notebook(notebook, filename):
    """Save notebook to file."""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    nb_path = project_root / 'notebooks' / filename

    with open(nb_path, 'w', encoding='utf-8') as f:
        nbf.write(notebook, f)
    print(f"[OK] Created: {nb_path}")


if __name__ == "__main__":
    print("Creating final notebooks (4 & 5)...")
    print("="*80)

    # Create Hyperparameter Optimization notebook
    print("\n3. Creating Hyperparameter Optimization notebook...")
    hpo_nb = create_hyperparameter_optimization_notebook()
    save_notebook(hpo_nb, '04_hyperparameter_optimization.ipynb')

    # Create Model Interpretation notebook
    print("\n4. Creating Model Interpretation notebook...")
    mi_nb = create_model_interpretation_notebook()
    save_notebook(mi_nb, '05_model_interpretation.ipynb')

    print("\n" + "="*80)
    print("[OK] All notebooks created successfully!")
    print("\nComplete notebook series:")
    print("  1. 01_eda.ipynb")
    print("  2. 02_feature_engineering.ipynb")
    print("  3. 03_baseline_models.ipynb")
    print("  4. 04_hyperparameter_optimization.ipynb")
    print("  5. 05_model_interpretation.ipynb")
    print("\nYou now have a complete end-to-end ML project!")
    print("\nNext steps:")
    print("1. Run: jupyter notebook")
    print("2. Execute notebooks in order (01 ‚Üí 05)")
    print("3. Start MLflow UI: mlflow ui")
    print("4. Review all outputs and visualizations")
