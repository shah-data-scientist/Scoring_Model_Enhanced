{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Baseline Models with MLflow Tracking\n",
    "## Credit Scoring Model Project\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Train multiple baseline models\n",
    "- Set up MLflow experiment tracking\n",
    "- Evaluate models using appropriate metrics for imbalanced data\n",
    "- Compare model performance\n",
    "- Select best baseline for optimization\n",
    "\n",
    "**Why Multiple Baselines?**\n",
    "Different algorithms have different strengths:\n",
    "- **Logistic Regression:** Simple, interpretable, fast (good baseline)\n",
    "- **Random Forest:** Handles non-linearity, robust to outliers\n",
    "- **XGBoost:** Powerful gradient boosting, often wins competitions\n",
    "- **LightGBM:** Fast, memory-efficient, great for large datasets\n",
    "\n",
    "**MLflow Tracking:**\n",
    "We'll log all experiments to compare models systematically. This is professional ML workflow!\n",
    "\n",
    "Let's build our models! \ud83d\ude80\n",
    "\n",
    "## \ud83d\udce6 Import Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# ML models\n",
    "from sklearn.dummy import DummyClassifier  # Reference baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Our utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.evaluation import (\n",
    "    evaluate_model,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_confusion_matrix,\n",
    "    compare_models,\n",
    "    plot_feature_importance\n",
    ")\n",
    "from src.model_training import train_and_evaluate_model\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"[OK] Libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcc2 Load Processed Data\n",
    "\n",
    "Load the data we prepared in the feature engineering notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "data_dir = Path('../data/processed')\n",
    "\n",
    "print(\"Loading processed datasets...\")\n",
    "X_train = pd.read_csv(data_dir / 'X_train.csv')\n",
    "X_val = pd.read_csv(data_dir / 'X_val.csv')\n",
    "y_train = pd.read_csv(data_dir / 'y_train.csv').squeeze()\n",
    "y_val = pd.read_csv(data_dir / 'y_val.csv').squeeze()\n",
    "\n",
    "print(f\"[OK] Data loaded!\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Training: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"  Validation: {y_val.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "print(f\"\\nFeature count: {X_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udd2c Setup MLflow Experiment Tracking\n",
    "\n",
    "**What MLflow Does:**\n",
    "- Automatically logs all your experiments\n",
    "- Stores parameters, metrics, and artifacts\n",
    "- Provides a UI to visualize and compare runs\n",
    "- Makes your work reproducible\n",
    "\n",
    "**To view experiments:**\n",
    "```bash\n",
    "# In a separate terminal, run:\n",
    "mlflow ui\n",
    "# Then open: http://localhost:5000\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name\n",
    "experiment_name = \"credit_scoring_baseline_models\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///../mlruns/mlflow.db\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get experiment ID\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"[OK] MLflow experiment set: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"\\nArtifacts will be stored in: {experiment.artifact_location}\")\n",
    "print(f\"\\nTo view experiments, run: mlflow ui\")\n",
    "print(f\"Then open: http://localhost:5000\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\ude80 Train Baseline Models\n",
    "\n",
    "We'll train 4 different models and compare them.\n",
    "\n",
    "**Model Selection Rationale:**\n",
    "\n",
    "**0. DummyClassifier (Reference Baseline)**\n",
    "   - Predicts majority class (always \"no default\")\n",
    "   - NO learning - just baseline to beat\n",
    "   - Shows minimum acceptable performance\n",
    "   - **Critical:** Any real model MUST beat this!\n",
    "\n",
    "1. **Logistic Regression**\n",
    "   - Simple linear model\n",
    "   - Fast to train\n",
    "   - Highly interpretable\n",
    "   - Good baseline to beat\n",
    "\n",
    "2. **Random Forest**\n",
    "   - Ensemble of decision trees\n",
    "   - Handles non-linear relationships\n",
    "   - Robust to outliers\n",
    "   - Built-in feature importance\n",
    "\n",
    "3. **XGBoost**\n",
    "   - Gradient boosting\n",
    "   - Often wins ML competitions\n",
    "   - Handles imbalanced data well\n",
    "   - Many hyperparameters to tune\n",
    "\n",
    "4. **LightGBM**\n",
    "   - Microsoft's gradient boosting\n",
    "   - Very fast and memory-efficient\n",
    "   - Great for large datasets\n",
    "   - Often comparable to XGBoost\n",
    "\n",
    "Let's train them all!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log feature names and count once for the experiment\n",
    "with mlflow.start_run(run_name=\"Experiment_Setup\", nested=True):\n",
    "    # Log feature names as an artifact\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    features_path = Path(\"artifacts\") / \"feature_names.txt\"\n",
    "    features_path.parent.mkdir(exist_ok=True)\n",
    "    with open(features_path, \"w\") as f:\n",
    "        for feature in feature_names:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    mlflow.log_artifact(features_path, artifact_path=\"features\")\n",
    "    print(f\"[OK] Logged {len(feature_names)} feature names to MLflow artifact 'features/feature_names.txt'\")\n",
    "\n",
    "    # Log number of features as a parameter\n",
    "    mlflow.log_param(\"num_features\", len(feature_names))\n",
    "    print(f\"[OK] Logged number of features ({len(feature_names)}) to MLflow parameter 'num_features'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. DUMMY CLASSIFIER (REFERENCE BASELINE)\n",
    "print(\"\\n\\n### 0. DUMMY CLASSIFIER (REFERENCE BASELINE) ###\\n\")\n",
    "print(\"This model always predicts the majority class (no default).\")\n",
    "print(\"Any real model MUST beat this to be useful!\\n\")\n",
    "\n",
    "dummy_params = {\n",
    "    'strategy': 'most_frequent',  # Always predict majority class\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "dummy_model = DummyClassifier(**dummy_params)\n",
    "dummy_metrics, dummy_trained = train_and_evaluate_model(\n",
    "    dummy_model, \"Dummy_Classifier\", dummy_params,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u26a0\ufe0f  IMPORTANT: This is the MINIMUM performance threshold!\")\n",
    "print(\"   Any real model with ROC-AUC < 0.50 is worse than random guessing!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LOGISTIC REGRESSION\n",
    "print(\"\\n\\n### 1. LOGISTIC REGRESSION ###\\n\")\n",
    "\n",
    "lr_params = {\n",
    "    'max_iter': 1000,\n",
    "    'class_weight': 'balanced',  # Handle imbalance\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "lr_model = LogisticRegression(**lr_params)\n",
    "lr_metrics, lr_trained = train_and_evaluate_model(\n",
    "    lr_model, \"Logistic_Regression\", lr_params,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RANDOM FOREST\n",
    "print(\"\\n\\n### 2. RANDOM FOREST ###\\n\")\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 50,\n",
    "    'min_samples_leaf': 20,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_metrics, rf_trained = train_and_evaluate_model(\n",
    "    rf_model, \"Random_Forest\", rf_params,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. XGBOOST\n",
    "print(\"\\n\\n### 3. XGBOOST ###\\n\")\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Handle imbalance\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "xgb_metrics, xgb_trained = train_and_evaluate_model(\n",
    "    xgb_model, \"XGBoost\", xgb_params,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LIGHTGBM\n",
    "print(\"\\n\\n### 4. LIGHTGBM ###\\n\")\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgbm_model = LGBMClassifier(**lgbm_params)\n",
    "lgbm_metrics, lgbm_trained = train_and_evaluate_model(\n",
    "    lgbm_model, \"LightGBM\", lgbm_params,\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcca Compare All Models\n",
    "\n",
    "Let's compare all our baseline models side-by-side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all results\n",
    "all_results = {\n",
    "    'Dummy_Classifier': dummy_metrics,\n",
    "    'Logistic_Regression': lr_metrics,\n",
    "    'Random_Forest': rf_metrics,\n",
    "    'XGBoost': xgb_metrics,\n",
    "    'LightGBM': lgbm_metrics\n",
    "}\n",
    "\n",
    "# Compare using our utility function\n",
    "comparison_df = compare_models(all_results, metric='roc_auc')\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('model_comparison.csv')\n",
    "print(\"\\n[OK] Comparison saved to model_comparison.csv\")\n",
    "\n",
    "# Calculate improvement over dummy baseline\n",
    "dummy_roc = dummy_metrics['roc_auc']\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT OVER DUMMY BASELINE\")\n",
    "print(\"=\"*80)\n",
    "for model_name, metrics in all_results.items():\n",
    "    if model_name != 'Dummy_Classifier':\n",
    "        improvement = ((metrics['roc_auc'] - dummy_roc) / dummy_roc) * 100\n",
    "        print(f\"{model_name:25s}: +{improvement:6.2f}% improvement over dummy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics_to_plot = ['roc_auc', 'pr_auc', 'f1', 'precision', 'recall']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    if metric in comparison_df.columns:\n",
    "        data = comparison_df[metric].sort_values(ascending=False)\n",
    "        \n",
    "        axes[idx].barh(range(len(data)), data.values, color='steelblue', alpha=0.8)\n",
    "        axes[idx].set_yticks(range(len(data)))\n",
    "        axes[idx].set_yticklabels(data.index)\n",
    "        axes[idx].set_xlabel(metric.upper().replace('_', '-'), fontsize=12)\n",
    "        axes[idx].set_title(f'{metric.upper().replace(\" \", \" \")} Comparison', \n",
    "                           fontsize=14, fontweight='bold')\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(data.values):\n",
    "            axes[idx].text(v + 0.005, i, f'{v:.4f}', \n",
    "                          va='center', fontsize=10)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Comparison visualization saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## \ud83d\udcdd Baseline Models Summary\n",
    "\n",
    "### \u2705 What We Accomplished\n",
    "\n",
    "1. **Trained 4 Baseline Models**\n",
    "   - Logistic Regression (simple baseline)\n",
    "   - Random Forest (ensemble method)\n",
    "   - XGBoost (gradient boosting)\n",
    "   - LightGBM (efficient gradient boosting)\n",
    "\n",
    "2. **MLflow Experiment Tracking**\n",
    "   - All runs logged automatically\n",
    "   - Parameters, metrics, and artifacts stored\n",
    "   - Compare runs visually in MLflow UI\n",
    "\n",
    "3. **Comprehensive Evaluation**\n",
    "   - ROC-AUC scores\n",
    "   - Precision-Recall curves\n",
    "   - Confusion matrices\n",
    "   - Feature importance (tree models)\n",
    "\n",
    "4. **Model Comparison**\n",
    "   - Side-by-side metrics\n",
    "   - Visual comparisons\n",
    "   - Identified best baseline\n",
    "\n",
    "### \ud83c\udfc6 Best Performing Model\n",
    "\n",
    "Based on ROC-AUC and PR-AUC scores:\n",
    "- **Best Model:** [Check comparison above]\n",
    "- **ROC-AUC:** [Value]\n",
    "- **PR-AUC:** [Value]\n",
    "- **F1-Score:** [Value]\n",
    "\n",
    "### \ud83d\udca1 Key Insights\n",
    "\n",
    "1. **Tree-based models** (RF, XGB, LightGBM) generally outperform Logistic Regression\n",
    "   - They can capture non-linear relationships\n",
    "   - Better handle feature interactions\n",
    "\n",
    "2. **Class imbalance handling** is critical\n",
    "   - Used `class_weight='balanced'` or `scale_pos_weight`\n",
    "   - Evaluated with appropriate metrics (ROC-AUC, PR-AUC, F1)\n",
    "\n",
    "3. **Feature importance** reveals key predictors\n",
    "   - Debt-to-income ratio likely important\n",
    "   - External credit scores matter\n",
    "   - Age and employment features contribute\n",
    "\n",
    "4. **Model complexity vs performance trade-off**\n",
    "   - Logistic Regression: Fast, interpretable, but lower performance\n",
    "   - Tree models: Higher performance, but less interpretable\n",
    "\n",
    "### \ud83c\udfaf Next Steps\n",
    "\n",
    "In the next notebook ([04_hyperparameter_optimization.ipynb](04_hyperparameter_optimization.ipynb)), we will:\n",
    "\n",
    "1. **Select Best Baseline**\n",
    "   - Choose the best performing model\n",
    "   - Or ensemble top models\n",
    "\n",
    "2. **Systematic Hyperparameter Tuning**\n",
    "   - Define search space\n",
    "   - Use GridSearchCV or RandomizedSearchCV\n",
    "   - Use StratifiedKFold cross-validation\n",
    "\n",
    "3. **Optimize for Target Metric**\n",
    "   - Focus on ROC-AUC or PR-AUC\n",
    "   - Consider business costs (FP vs FN)\n",
    "\n",
    "4. **Log All Optimization Runs**\n",
    "   - Track in MLflow\n",
    "   - Compare optimization strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work! You now have solid baseline models! \ud83c\udf89**\n",
    "\n",
    "### \ud83d\udcca To View Your Experiments:\n",
    "\n",
    "```bash\n",
    "# In terminal, run:\n",
    "mlflow ui\n",
    "\n",
    "# Then open in browser:\n",
    "http://localhost:5000\n",
    "```\n",
    "\n",
    "In the MLflow UI, you can:\n",
    "- Compare all runs side-by-side\n",
    "- Sort by metrics\n",
    "- View all plots and artifacts\n",
    "- Download models\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** These are baselines! We'll improve them significantly in the next notebook through hyperparameter optimization! \ud83d\ude80\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}