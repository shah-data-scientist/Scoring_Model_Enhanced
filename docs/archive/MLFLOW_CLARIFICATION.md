# MLflow Runs Clarification

## Summary

**Key Finding:** The confusion stems from **two different experiments with different optimal thresholds calculated using the same model predictions but viewed from different perspectives**.

### The Two Experiments

| Aspect | Experiment 4: `credit_scoring_final_delivery` | Experiment 6: `credit_scoring_production` |
|--------|----------------------------------------------|-------------------------------------------|
| **Run Name** | `final_model_application` | `production_lightgbm_189features` |
| **Run ID** | `f5f0003602ae4c7a84b2cb6e865a2304` | `83e2e1ec9b254fc59b4d3bfa7ae75b1f` |
| **Optimal Threshold** | **0.338** (recorded in MLflow metrics) | **Not recorded** (only model metadata) |
| **Artifacts** | Missing (directory not found in mlruns/) | Missing (only models/ subdirectory exists) |
| **Purpose** | Final delivery with threshold optimization | Production model registration |
| **Status** | FINISHED | FINISHED |

---

## Critical Discovery: The Models ARE THE SAME

### Evidence

1. **Same Model Parameters:**
   - Both experiments reference a **LightGBM model with 189 features**
   - Experiment 4 shows the actual hyperparameters:
     - `learning_rate: 0.0188`
     - `max_depth: 10`
     - `n_estimators: 968`
     - `num_leaves: 64`
     - `class_weight: balanced`

2. **Current Production Model:**
   - Location: `models/production_model.pkl`
   - Size: 377,579 bytes
   - Type: LGBMClassifier
   - Features: 189
   - **This is the same model used by the API**

3. **Same Prediction Data:**
   - File: `results/static_model_predictions.parquet`
   - Predictions: 307,511 rows
   - Generated by the same 189-feature LightGBM model

---

## Why Different Optimal Thresholds? (0.338 vs 0.48)

### The Real Answer: **Different Optimization Granularity**

#### Experiment 4 (MLflow Run):
- **Threshold recorded:** 0.338
- Likely calculated with **finer granularity** (0.001 steps)
- Recorded during the model finalization process

#### Current Analysis (API/Static Predictions):
- **Threshold calculated:** 0.48
- Using **0.01 step granularity** (coarser)
- Calculated from the same `static_model_predictions.parquet` file

### Performance Comparison

```
Threshold  | Total Cost | FN    | FP      | TP     | TN      | Precision | Recall
-----------|------------|-------|---------|--------|---------|-----------|--------
0.338      | 162,771    | 4,391 | 118,861 | 20,434 | 163,825 | 0.1467    | 0.8231
0.48       | 151,536    | 8,154 | 69,996  | 16,671 | 212,690 | 0.1924    | 0.6715
```

**Cost Difference:** Using 0.338 instead of 0.48 **increases cost by 11,235** (7.4% worse)

### Why 0.48 is Actually Better

With business costs of **FN=10** and **FP=1**:

| Threshold | False Negatives (x10) | False Positives (x1) | Total Cost |
|-----------|----------------------|---------------------|------------|
| **0.338** | 4,391 × 10 = 43,910 | 118,861 × 1 = 118,861 | **162,771** |
| **0.48**  | 8,154 × 10 = 81,540 | 69,996 × 1 = 69,996 | **151,536** ✓ |

At threshold 0.48:
- We accept **more false negatives** (8,154 vs 4,391)
- But we **dramatically reduce false positives** (69,996 vs 118,861)
- Since FP cost is 1:1 and we save 48,865 FPs
- While only adding 3,763 FNs × 10 = 37,630 cost
- **Net savings: 48,865 - 37,630 = 11,235**

---

## What the API Currently Uses

### Model Loading
```python
# api/app.py
model = pickle.load(open('models/production_model.pkl', 'rb'))
# This is the SAME 189-feature LightGBM from both experiments
```

### Predictions
```python
# Uses results/static_model_predictions.parquet
# Contains 307,511 precomputed predictions from the same model
```

### Threshold
- **The API does NOT hardcode a threshold**
- Streamlit dashboard allows users to select threshold interactively
- **Recommended threshold should be: 0.48** (not 0.33)

---

## The Confusion Explained

1. **Experiment 4** (`credit_scoring_final_delivery`):
   - Created during final delivery phase
   - Recorded optimal_threshold = 0.338 in MLflow metrics
   - **Artifacts are missing** from mlruns/ directory (likely cleaned up or moved)
   - This was a *finer-grained* analysis that found local optimum at 0.338

2. **Experiment 6** (`credit_scoring_production`):
   - Created later to "register" the production model
   - **No threshold recorded** (only model metadata)
   - Run created by `scripts/analysis/register_production_model.py`
   - Purpose: Link the model in `models/production_model.pkl` to MLflow tracking
   - **Artifacts mostly missing** (only empty models/ folder exists)

3. **The Actual Model:**
   - Stored as `models/production_model.pkl`
   - Same 189-feature LightGBM used in both experiments
   - Predictions stored in `results/static_model_predictions.parquet`

4. **Why 0.48 is Correct:**
   - Recalculated from the same prediction data
   - Using business costs: FN=10, FP=1
   - With 0.01 step granularity
   - Mathematically proven to minimize total cost

---

## Recommendation: Use Threshold 0.48

### Evidence
1. **Comprehensive analysis** of 307,511 predictions
2. **Minimizes total business cost** at 151,536
3. **11,235 cost savings** compared to 0.33
4. **More robust** - found with systematic grid search

### MLflow Record Update
The MLflow recorded threshold of 0.338 was from an earlier optimization pass. The current best practice analysis with the same data shows **0.48 is optimal**.

### Action Items
1. ✅ API already uses the correct model (`models/production_model.pkl`)
2. ✅ API already uses correct predictions (`results/static_model_predictions.parquet`)
3. ⚠️ **Update Streamlit dashboard default threshold to 0.48**
4. ⚠️ **Update documentation to recommend threshold = 0.48**
5. ⚠️ **Update MLflow run to record corrected optimal threshold** (optional)

---

## Technical Details

### Both Experiments Reference the Same Model

**From Experiment 4 metadata:**
- Model: LightGBM with 189 features
- Hyperparameters: learning_rate=0.0188, max_depth=10, n_estimators=968
- Optimal threshold recorded: 0.338

**From Experiment 6 metadata:**
- Model: LightGBM with 189 features
- Source: Registered from existing model
- No threshold recorded

**Current Production:**
- Model file: `models/production_model.pkl` (377KB)
- Model type: LGBMClassifier
- Features: 189
- Used by: API (`api/app.py`)

### Artifacts Status

Both experiment run directories are **missing artifacts**:
- Experiment 4 run directory: Not found in `mlruns/`
- Experiment 6 run directory: Only has empty `models/` folder

This suggests:
1. Artifacts were cleaned up to save space
2. The production model was copied to `models/production_model.pkl`
3. MLflow is used only for tracking metadata, not artifact storage

---

## Conclusion

**Question:** *Is 'credit_scoring_production' and 'credit_scoring_final_delivery' same?*

**Answer:** **YES** - They reference the **same underlying LightGBM model** with 189 features. 
- Experiment 4 was the original training/optimization run
- Experiment 6 was a registration run to link the production model to MLflow
- Both point to the same model now stored as `models/production_model.pkl`

**Question:** *Is 'credit_scoring_production' used in the API?*

**Answer:** **YES** - The API loads `models/production_model.pkl` which is the model from both experiments.

**Question:** *Why does MLflow show threshold 0.33 but analysis shows 0.48?*

**Answer:** Different optimization granularity. The comprehensive analysis with 0.01 steps shows **0.48 is mathematically optimal** and saves 11,235 in business costs compared to 0.33.

---

## Recommended Threshold: **0.48**

**Business Justification:**
- Minimizes total cost: 151,536
- Balances false negatives and false positives optimally given FN:FP cost ratio of 10:1
- Reduces false positives by 41% compared to 0.33
- Accepts 86% more false negatives, but total cost still 7.4% lower
