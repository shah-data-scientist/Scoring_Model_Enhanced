"""Diagnose why test set predictions differ."""
import pandas as pd
import pickle
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent

# Sample test ID with large difference
test_id = 383433  # API=29.71%, Submission=95.38%, diff=65.67%

print("="*80)
print(f"DIAGNOSING TEST APPLICATION: {test_id}")
print("="*80)

# Check if this ID is in precomputed features (it shouldn't be, it's a test ID)
parquet_df = pd.read_parquet(PROJECT_ROOT / 'data' / 'processed' / 'precomputed_features.parquet')
in_precomputed = test_id in parquet_df['SK_ID_CURR'].values

print(f"\nIs {test_id} in precomputed_features.parquet (training data)? {in_precomputed}")

if in_precomputed:
    print("  [ERROR] Test ID found in training data! This shouldn't happen.")
    print("  The test and train sets should be completely separate.")
else:
    print("  [OK] Test ID not in training data (as expected)")
    print("  This means the batch API processed it through the FULL pipeline")

# Check the submission.csv prediction
submission = pd.read_csv(PROJECT_ROOT / 'results' / 'submission.csv')
submission_pred = submission[submission['SK_ID_CURR'] == test_id]

if len(submission_pred) > 0:
    sub_prob = submission_pred['TARGET'].values[0]
    print(f"\nSubmission.csv prediction for {test_id}: {sub_prob:.4f} ({sub_prob*100:.1f}%)")
else:
    print(f"\n[ERROR] {test_id} not found in submission.csv")

# Load the API prediction
api_preds = pd.read_csv(PROJECT_ROOT / 'results' / 'end_user_test_predictions.csv')
api_pred = api_preds[api_preds['sk_id_curr'] == test_id]

if len(api_pred) > 0:
    api_prob = api_pred['probability'].values[0]
    print(f"Batch API prediction for {test_id}: {api_prob:.4f} ({api_prob*100:.1f}%)")
    print(f"\nDifference: {abs(api_prob - sub_prob):.4f} ({abs(api_prob - sub_prob)*100:.1f}%)")
else:
    print(f"\n[ERROR] {test_id} not found in API predictions")

print("\n" + "="*80)
print("POSSIBLE CAUSES OF DISCREPANCY")
print("="*80)

print("""
1. Different preprocessing logic between:
   - apply_best_model.py (used to create submission.csv)
   - api/preprocessing_pipeline.py (used by batch API)

2. Feature engineering differences:
   - Domain features may be calculated differently
   - Aggregations may use different logic
   - Missing value handling may differ

3. Model version mismatch:
   - Submission.csv might use a different model version
   - Check if production_model.pkl is the same model used for submission

4. Data loading differences:
   - How auxiliary data is filtered and joined
   - How missing auxiliary data is handled

RECOMMENDATION:
Compare the actual features generated by both pipelines for this test ID.
The features should be IDENTICAL if using the same preprocessing logic.
""")

print("="*80)
print("CHECKING MODEL CONSISTENCY")
print("="*80)

# Load the model
with open(PROJECT_ROOT / 'models' / 'production_model.pkl', 'rb') as f:
    model = pickle.load(f)

print(f"Model type: {type(model).__name__}")
print(f"Number of features expected: {len(model.feature_name_)}")

# Check if we can load the test features that were used for submission
test_features_path = PROJECT_ROOT / 'data' / 'processed' / 'X_test.csv'
if test_features_path.exists():
    print(f"\n[FOUND] X_test.csv exists - this contains the features used for submission.csv")
    X_test = pd.read_csv(test_features_path)
    print(f"X_test shape: {X_test.shape}")

    # Check if we have test IDs mapping
    test_ids_path = PROJECT_ROOT / 'data' / 'processed' / 'test_ids.csv'
    if test_ids_path.exists():
        test_ids = pd.read_csv(test_ids_path)
        print(f"test_ids shape: {test_ids.shape}")

        # Find the row for our test ID
        if test_id in test_ids['SK_ID_CURR'].values:
            idx = test_ids[test_ids['SK_ID_CURR'] == test_id].index[0]
            print(f"\n[FOUND] Test ID {test_id} is at index {idx} in X_test.csv")

            # Make prediction with the test features
            test_row = X_test.iloc[idx].values.reshape(1, -1)
            direct_prob = model.predict_proba(test_row)[0, 1]

            print(f"\nDirect prediction using X_test.csv: {direct_prob:.4f} ({direct_prob*100:.1f}%)")
            print(f"Submission.csv value: {sub_prob:.4f} ({sub_prob*100:.1f}%)")
            print(f"Match: {abs(direct_prob - sub_prob) < 0.0001}")

            if abs(direct_prob - sub_prob) < 0.0001:
                print("\n[CONCLUSION] X_test.csv features produce correct predictions")
                print("The batch API preprocessing pipeline is producing DIFFERENT features")
                print("than what's in X_test.csv for test applications.")
        else:
            print(f"\n[ERROR] Test ID {test_id} not found in test_ids.csv")
    else:
        print("[WARNING] test_ids.csv not found")
else:
    print(f"\n[WARNING] X_test.csv not found at {test_features_path}")
    print("Cannot verify the test features used for submission.csv")

print("\n" + "="*80)
